{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39da6eed-5d7b-40fc-82fb-97b8587b78f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand, randn\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee1d2599-b26e-41dc-b628-32949ec56459",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder\\\n",
    "        .appName(\"PythonWordCount\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edebdf00-2dcf-4a5e-ace7-7b747c0d4f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# rows:  1000000\n",
      "+---+-------------------+--------------------+\n",
      "| id|            uniform|              normal|\n",
      "+---+-------------------+--------------------+\n",
      "|  0| 0.1709497137955568| -0.8664700627108758|\n",
      "|  1| 0.8051143958005459| -0.5970491018333267|\n",
      "|  2| 0.5775925576589018| 0.18267161219540898|\n",
      "|  3| 0.9476047869880925| -1.8497305679917546|\n",
      "|  4|    0.2093704977577|  0.9410417279045351|\n",
      "|  5|0.36664222617947817| -0.6516475674670159|\n",
      "|  6| 0.8078688178371882|  0.5901002135239671|\n",
      "|  7| 0.7135143433452461|  -1.850241871360443|\n",
      "|  8| 0.7195325566306053| 0.09176896733073023|\n",
      "|  9|0.31335292311175456|-0.38605118617831075|\n",
      "| 10| 0.8062503712025726|  1.2134544166783332|\n",
      "| 11|0.10814914646176654| -1.0757702531630617|\n",
      "| 12| 0.3362232980701172| 0.04961226872064977|\n",
      "| 13| 0.8133304803837667|  -0.768259602441542|\n",
      "| 14|0.47649428738170896|  0.2911293146907403|\n",
      "| 15|  0.524728096293865|-0.33406080411047484|\n",
      "| 16| 0.9701253460019921|  1.3607097640771781|\n",
      "| 17| 0.6232167713919952|  0.5986772981082732|\n",
      "| 18| 0.5089687568245219|-0.35158579838711623|\n",
      "| 19| 0.5467504094508642| -0.9115825072509143|\n",
      "+---+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlc = SQLContext(spark.sparkContext)\n",
    "\n",
    "df = (sqlc.range(0, 1000 * 1000).withColumn('uniform', rand(seed=10)).withColumn('normal', randn(seed=27)))\n",
    "\n",
    "print('# rows: ', df.count())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03c2910d-04ad-4064-8bd0-3a6437489fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+--------------------+\n",
      "|summary|                id|             uniform|              normal|\n",
      "+-------+------------------+--------------------+--------------------+\n",
      "|  count|           1000000|             1000000|             1000000|\n",
      "|   mean|          499999.5|  0.4997785318606761|6.545992003465573E-4|\n",
      "| stddev|288675.27893234405|  0.2887560412263698|  1.0003498848232582|\n",
      "|    min|                 0|2.710561290975022E-7|  -4.949492960499273|\n",
      "|    max|            999999|  0.9999998822463074|   4.474351963425938|\n",
      "+-------+------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05c2ad8a-00b6-45d8-a02d-4b6e65106b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|summary|             uniform|              normal|\n",
      "+-------+--------------------+--------------------+\n",
      "|  count|             1000000|             1000000|\n",
      "|   mean|  0.4997785318606761|6.545992003465573E-4|\n",
      "| stddev|  0.2887560412263698|  1.0003498848232582|\n",
      "|    min|2.710561290975022E-7|  -4.949492960499273|\n",
      "|    max|  0.9999998822463074|   4.474351963425938|\n",
      "+-------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe('uniform', 'normal').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0545372b-20f5-4ae1-966e-0a956e12dc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+------------------+\n",
      "|      avg(uniform)|        min(uniform)|      max(uniform)|\n",
      "+------------------+--------------------+------------------+\n",
      "|0.4997785318606761|2.710561290975022E-7|0.9999998822463074|\n",
      "+------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean, min, max\n",
    "\n",
    "df.select([mean('uniform'), min('uniform'), max('uniform')]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8b89ba4-08f2-4a67-b345-534b076a6d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00011441526053927191"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Covariância\n",
    "sqlContext = SQLContext(spark.sparkContext)\n",
    "\n",
    "df = sqlContext.range(0, 1000 * 1000).withColumn('rand1', rand(seed=10)).withColumn('rand2', rand(seed=27))\n",
    "\n",
    "df.stat.cov('rand1', 'rand2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9f4ed1c-d78a-4d82-b833-341ea8518b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cov(rand2, rand2):  0.08339799711775965\n",
      "+-------------------+\n",
      "|    variance(rand2)|\n",
      "+-------------------+\n",
      "|0.08339799711775966|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Covariância\n",
    "sqlContext = SQLContext(spark.sparkContext)\n",
    "\n",
    "df = sqlContext.range(0, 1000 * 1000).withColumn('rand1', rand(seed=10)).withColumn('rand2', rand(seed=27))\n",
    "\n",
    "print('cov(rand2, rand2): ', df.stat.cov('rand2', 'rand2'))\n",
    "\n",
    "df.agg({'rand2': 'variance'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0269780d-a325-4346-9ad9-7bca227b50d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cor(rand2, rand2):  1.0\n",
      "cor(rand1, rand2):  0.00137206619523886\n"
     ]
    }
   ],
   "source": [
    "# Correlação\n",
    "sqlContext = SQLContext(spark.sparkContext)\n",
    "\n",
    "df = sqlContext.range(0, 1000 * 1000).withColumn('rand1', rand(seed=10)).withColumn('rand2', rand(seed=27))\n",
    "\n",
    "print('cor(rand2, rand2): ', df.stat.corr('rand2', 'rand2'))\n",
    "\n",
    "print('cor(rand1, rand2): ', df.stat.corr('rand1', 'rand2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8eec9fa6-fb46-4956-a7ce-a333402a712b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "| name|   color|\n",
      "+-----+--------+\n",
      "|Pedro|   verde|\n",
      "|Maria| amarelo|\n",
      "| João|    rosa|\n",
      "|Pedro|vermelho|\n",
      "|Maria|   preto|\n",
      "| João|   verde|\n",
      "|Pedro| amarelo|\n",
      "|Maria|    rosa|\n",
      "| João|vermelho|\n",
      "|Pedro|   preto|\n",
      "+-----+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tabela de Contigência\n",
    "names = [\"Pedro\", \"Maria\", \"João\"]\n",
    "colors = [\"verde\", \"amarelo\", \"rosa\", \"vermelho\", \"preto\"]\n",
    "df = sqlContext.createDataFrame([(names[i % 3], colors[i % 5]) for i in range(100)], [\"name\", \"color\"])\n",
    "\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b5268ed-ba8c-4b50-9949-6204bb8d1091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----+----+-----+--------+\n",
      "|name_color|amarelo|preto|rosa|verde|vermelho|\n",
      "+----------+-------+-----+----+-----+--------+\n",
      "|      João|      6|    6|   7|    7|       7|\n",
      "|     Maria|      7|    7|   7|    6|       6|\n",
      "|     Pedro|      7|    7|   6|    7|       7|\n",
      "+----------+-------+-----+----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Contagem da frequência de pares e valores de duas colunas\n",
    "df.stat.crosstab(\"name\", \"color\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "290ad0ce-2cd1-40f6-8f15-98571b271dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "|  1|  2|  3|\n",
      "|  1|  2|  1|\n",
      "|  1|  2|  3|\n",
      "|  3|  6|  3|\n",
      "|  1|  2|  3|\n",
      "|  5| 10|  1|\n",
      "|  1|  2|  3|\n",
      "|  7| 14|  3|\n",
      "|  1|  2|  3|\n",
      "|  9| 18|  1|\n",
      "+---+---+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Itens frequentes\n",
    "df = sqlContext.createDataFrame([(1, 2, 3) if i % 2 == 0 else (i, 2 * i, i % 4) for i in range(100)], [\"a\", \"b\", \"c\"])\n",
    "\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e036af3-5c39-4858-ac2c-bca4211997f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(a_freqItems=[11, 1], b_freqItems=[2, 22], c_freqItems=[1, 3])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = df.stat.freqItems([\"a\", \"b\", \"c\"], 0.4)\n",
    "\n",
    "freq.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "507193e8-3eb5-4a89-a13e-00f137babafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(ab_freqItems=[Row(a=11, b=22), Row(a=1, b=2)])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import struct\n",
    "\n",
    "freq = df.withColumn('ab', struct('a', 'b')).stat.freqItems(['ab'], 0.4)\n",
    "freq.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c5b7211c-3faa-4596-80c9-a7f402718748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+\n",
      "|            uniform|  DEGREES(uniform)|     cos^2 + sin^2|\n",
      "+-------------------+------------------+------------------+\n",
      "| 0.5367821013180484|30.755348923687915|               1.0|\n",
      "|0.10747087445354876| 6.157627526768682|               1.0|\n",
      "| 1.1475525508626785| 65.74991793390322|               1.0|\n",
      "|  1.310955978808693|  75.1122447131799|               1.0|\n",
      "| 3.1083266315458262|178.09399733569154|0.9999999999999999|\n",
      "| 0.5165986402305565|29.598921787408102|               1.0|\n",
      "| 0.5696528438969835| 32.63870374292187|0.9999999999999999|\n",
      "| 1.5573024855692674| 89.22685984835181|0.9999999999999999|\n",
      "| 3.0450071328478523| 174.4660572994135|               1.0|\n",
      "|0.23646103537894467|13.548219346507173|               1.0|\n",
      "+-------------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/functions.py:520: FutureWarning: Deprecated in 2.1, use degrees instead.\n",
      "  warnings.warn(\"Deprecated in 2.1, use degrees instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Funções matemáticas\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = sqlContext.range(0, 10).withColumn('uniform', rand(seed=10) * 3.14)\n",
    "\n",
    "df.select(\n",
    "    'uniform',\n",
    "    toDegrees('uniform'),\n",
    "    (pow(cos(df['uniform']), 2) + pow(sin(df['uniform']), 2)). \\\n",
    "    alias(\"cos^2 + sin^2\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d536ca-e4c2-494f-bb46-65f7c2ab6b40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
